Model editing techniques allow for targeted updates to the factual knowledge of Large Language Models (LLMs) without expensive retraining. While methods like Rank-One Model Editing (\rome) have proven effective for updating encyclopedic facts (\eg ``Paris is in France''), their applicability to modifying fundamental logical or arithmetic rules remains underexplored. In this work, we investigate the limits of knowledge isolation by attempting to edit \gptxl to believe the logical falsehood ``$2+2=5$'' while preserving other arithmetic capabilities. We find that current editing techniques fail to produce a coherent behavioral change: while \rome successfully maximizes the probability of the target token ``5'', the model's generation reverts to the prior ``4'' during inference. More critically, the edit causes catastrophic interference in neighboring arithmetic operations (\eg ``$2+3$'' becomes ``6'', ``$4+4$'' becomes ``10''), while leaving general linguistic capabilities largely intact. Our results suggest that arithmetic knowledge in LLMs is stored in a dense, procedural manifold rather than as isolated key-value pairs, making it resistant to standard rank-one updates.