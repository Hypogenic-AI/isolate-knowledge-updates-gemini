We showed that standard model editing techniques cannot cleanly isolate arithmetic updates. Updating \gptxl to believe ``$2+2=5$'' failed to alter generation and caused catastrophic interference in related math tasks, despite preserving general knowledge. Future work should investigate if multi-layer editing (\memit) or constrained fine-tuning can better handle procedural knowledge updates.